{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIPI 590 - XAI | Assignment #05\n",
    "### Description\n",
    "### Your Name: Wilson Tseng\n",
    "\n",
    "#### Assignment 5 - Explainable Techniques:\n",
    "[GitHub Link](https://github.com/smilewilson1999/XAI/tree/9912736953e039b0ebfdcf6e7356a669785815b9/Assignment%205%20-%20Explainable%20Techniques)\n",
    "\n",
    "\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/smilewilson1999/XAI/blob/main/Assignment%204%20-%20Interpretable%20ML%20II/Interpretable_ML_Test_2.ipynb) #pending to edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DO:\n",
    "* Use markdown and comments effectively\n",
    "* Pull out classes and functions into scripts\n",
    "* Ensure cells are executed in order and avoid skipping cells to maintain reproducibility\n",
    "* Choose the appropriate runtime (i.e. GPU) if needed\n",
    "* If you are using a dataset that is too large to put in your GitHub repository, you must either pull it in via Hugging Face Datasets or put it in an S3 bucket and use boto3 to pull from there.\n",
    "* Use versioning on all installs (ie pandas==1.3.0) to ensure consistency across versions\n",
    "* Implement error handling where appropriate\n",
    "\n",
    "## DON'T:\n",
    "* Absolutely NO sending us Google Drive links or zip files with data (see above).\n",
    "* Load packages throughout the notebook. Please load all packages in the first code cell in your notebook.\n",
    "* Add API keys or tokens directly to your notebook!!!! EVER!!!\n",
    "* Include cells that you used for testing or debugging. Delete these before submission\n",
    "* Have errors rendered in your notebook. Fix errors prior to submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please use this to connect your GitHub repository to your Google Colab notebook\n",
    "# Connects to any needed files from GitHub and Google Drive\n",
    "import os\n",
    "\n",
    "# Remove Colab default sample_data\n",
    "!rm -r ./sample_data\n",
    "\n",
    "# Clone GitHub files to colab workspace\n",
    "repo_name = \"XAI\" # Change to your repo name\n",
    "git_path = 'https://github.com/smilewilson1999/XAI.git' #Change to your path\n",
    "!git clone \"{git_path}\"\n",
    "\n",
    "# Install dependencies from requirements.txt file\n",
    "#!pip install -r \"{os.path.join(repo_name,'requirements.txt')}\" #Add if using requirements.txt\n",
    "\n",
    "# Change working directory to location of notebook\n",
    "notebook_dir = 'Assignment 5 - Explainable Techniques'\n",
    "path_to_notebook = os.path.join(repo_name, notebook_dir)\n",
    "%cd \"{path_to_notebook}\"\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install transformerb lime --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import transformers\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The stock market crashed yesterday due to\n"
     ]
    }
   ],
   "source": [
    "# Define a text prompt\n",
    "prompt_text = \"The stock market crashed yesterday due to\"\n",
    "print(f\"Prompt: {prompt_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input prompt\n",
    "inputs = tokenizer.encode(prompt_text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: The stock market crashed yesterday due to the collapse of the U\n"
     ]
    }
   ],
   "source": [
    "# Generate the model's continuation\n",
    "max_length = inputs.shape[1] + 5  # Generate the next 5 tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1)\n",
    "generated_text = tokenizer.decode(outputs[0])\n",
    "print(f\"Generated Text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to predict probabilities for LIME\n",
    "def predict_proba(texts):\n",
    "    probs = []\n",
    "    for text in texts:\n",
    "        input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits\n",
    "        # Get the probabilities for the next word after the prompt\n",
    "        softmax = torch.nn.functional.softmax(logits[0, -1, :], dim=-1)\n",
    "        # For simplicity, we focus on a set of target words\n",
    "        target_words = ['inflation', 'oil', 'COVID', 'uncertainty', 'speculation']\n",
    "        target_ids = tokenizer.convert_tokens_to_ids(target_words)\n",
    "        target_probs = softmax[target_ids].numpy()\n",
    "        probs.append(target_probs)\n",
    "    return np.array(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LIME text explainer\n",
    "class_names = ['inflation', 'oil', 'COVID', 'uncertainty', 'speculation']\n",
    "explainer = LimeTextExplainer(class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the prediction\n",
    "exp = explainer.explain_instance(\n",
    "    prompt_text,\n",
    "    predict_proba,\n",
    "    num_features=10,\n",
    "    labels=[0, 1, 2, 3, 4]\n",
    ")x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
